{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2885d62",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444e915",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9b7163",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8136ae26",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefab760",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f554362",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "##### Hotel rates with respect to customer segment type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11159437",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.boxplot(data=df, x='market_segment', y='avg_daily_rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e92aeda",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "q1 = df.avg_daily_rate.quantile(0.25)\n",
    "q3 = df.avg_daily_rate.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "ul = q3 + 1.5 * iqr\n",
    "ul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97917a77",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df['avg_daily_rate'] >= 500, 'avg_daily_rate'] = ul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10ea26",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.boxplot(data=df, x='market_segment', y='avg_daily_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32dd3c1",
   "metadata": {},
   "source": [
    "* Rooms booked Online and Direct have high variation in the prices.\n",
    "* Complementary type segment has the very low price.\n",
    "* While Coroprate, Offline and Groups segments have the almost the similar room prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ae92b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,3))\n",
    "sns.countplot(data = df, x = 'market_segment', order=df['market_segment'].value_counts().sort_values(ascending=False).index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bdf4ac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "stacked = (pd.crosstab(df['market_segment'], df['is_canceled'], normalize='index')*100)\n",
    "stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f24f08",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "stacked.plot(kind = 'bar', stacked = True)\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xticks(rotation=20)\n",
    "plt.ylabel(\"Percentage Cancellation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2373113",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "* Complementary and Direct booking are the common customer with the lowest percentage of cancellation rate, while the Groups customers along with Online and Offline customers have the higher cancallation rate.\n",
    "* As per the graph show above, companies should target the Online and Offline customers in addition to Groups booking customers.\n",
    "* Corporate and Aviation customers also have the higher cancellation rates that hotel should focus on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fbd69f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "stacked_deposit = (pd.crosstab(df['deposit_type'], df['is_canceled'], normalize='index')*100)\n",
    "stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd6e671",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "stacked_deposit.plot(kind = 'bar', stacked = True)\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xticks(rotation = 0)\n",
    "plt.ylabel(\"Percentage Cancellation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725ca39a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "* Non-Refundable deposit type results almost 100% cancellation percentage, while the Non-Deposit and Refundable type retain almost 75% of reservation.\n",
    "* The importance of the deposit type in reservation management is immense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809af26d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "stacked_repeated = (pd.crosstab(df['is_repeated_guest'], df['is_canceled'], normalize='index')*100)\n",
    "stacked_repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a1d4ec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "stacked_repeated.plot(kind = 'bar', stacked = True)\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xticks(rotation=0)\n",
    "plt.xlabel('Repeated Guest')\n",
    "plt.ylabel(\"Percentage Cancellation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2fc5cb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "* The rate of cancellation of non-repeated guest is higher than repeated guest:\n",
    "    - It's important for management to focus on improving the experience of non-repeated guest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434e6cf9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "grup = df.groupby('arrival_date_month', as_index=False).agg({'avg_daily_rate':'mean'})\n",
    "grup['arrival_date_month'] = pd.Categorical(grup['arrival_date_month'], categories=months, ordered=True)\n",
    "grup = grup.sort_values('arrival_date_month')\n",
    "grup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8971571d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "sns.lineplot(x = grup['arrival_date_month'], y = grup['avg_daily_rate'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb5f24e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "##### 3. Outlier Detection and Treatment (IQR Method with Winsorization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b576d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "##### 4. Association Rule Mining using Apriori Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad91f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Apriori Algorithm for Association Rule Mining\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Objective: Find frequent patterns and association rules in hotel booking data\")\n",
    "print(\"Focus: Discover which combinations of features are associated with cancellations\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3452c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "##### 4.2. Data Preparation for Apriori Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c9430a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a copy of the original dataframe for Apriori analysis\n",
    "df_apriori = pd.read_csv(r\"C:\\Users\\91904\\OneDrive\\Desktop\\dm-2\\Hotel-Reservation-Cancellation-Prediction\\hotel_booking.csv\")\n",
    "\n",
    "# Select key categorical and binned numerical features for association rule mining\n",
    "# Convert numerical features to categorical bins for better rule discovery\n",
    "\n",
    "# Lead Time Binning\n",
    "df_apriori['lead_time_category'] = pd.cut(df_apriori['lead_time'], \n",
    "                                           bins=[0, 30, 90, 180, 365, float('inf')],\n",
    "                                           labels=['Very_Short', 'Short', 'Medium', 'Long', 'Very_Long'])\n",
    "\n",
    "# Average Daily Rate Binning\n",
    "df_apriori['adr_category'] = pd.cut(df_apriori['avg_daily_rate'],\n",
    "                                   bins=[0, 50, 100, 150, 200, float('inf')],\n",
    "                                   labels=['Low', 'Medium', 'High', 'Very_High', 'Premium'])\n",
    "\n",
    "# Stays Binning\n",
    "df_apriori['total_stays'] = df_apriori['stays_in_weekend_nights'] + df_apriori['stays_in_week_nights']\n",
    "df_apriori['total_stays_category'] = pd.cut(df_apriori['total_stays'],\n",
    "                                            bins=[0, 2, 5, 10, float('inf')],\n",
    "                                            labels=['Short_Stay', 'Medium_Stay', 'Long_Stay', 'Extended_Stay'])\n",
    "\n",
    "# Days in Waiting List Binning\n",
    "df_apriori['waiting_list_category'] = pd.cut(df_apriori['days_in_waiting_list'],\n",
    "                                              bins=[-1, 0, 30, 100, float('inf')],\n",
    "                                              labels=['No_Wait', 'Short_Wait', 'Medium_Wait', 'Long_Wait'])\n",
    "\n",
    "# Convert cancellation to categorical\n",
    "df_apriori['is_canceled_cat'] = df_apriori['is_canceled'].map({0: 'Not_Canceled', 1: 'Canceled'})\n",
    "\n",
    "# print(\"Data prepared for Apriori Algorithm\")\n",
    "# print(f\"Total records: {len(df_apriori)}\")\n",
    "# print(\"\\nSelected features for association rule mining:\")\n",
    "# print(\"  - Hotel type\")\n",
    "# print(\"  - Market segment\")\n",
    "# print(\"  - Deposit type\")\n",
    "# print(\"  - Meal type\")\n",
    "# print(\"  - Is repeated guest\")\n",
    "# print(\"  - Lead time category\")\n",
    "# print(\"  - ADR category\")\n",
    "# print(\"  - Total stays category\")\n",
    "# print(\"  - Waiting list category\")\n",
    "# print(\"  - Cancellation status\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6019bfb4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "##### 4.3. Creating Transaction Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8d5c18",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Select key features for Apriori (reduced set to avoid memory issues)\n",
    "features_for_apriori = [\n",
    "    'hotel',\n",
    "    'market_segment',\n",
    "    'deposit_type',\n",
    "    'meal',\n",
    "    'is_repeated_guest',\n",
    "    'lead_time_category',\n",
    "    'adr_category',\n",
    "    'total_stays_category',\n",
    "    'is_canceled_cat'\n",
    "]\n",
    "\n",
    "# Use a sample of data to reduce memory usage (20% sample = ~24,000 records)\n",
    "sample_size = int(len(df_apriori) * 0.2)\n",
    "df_apriori_sample = df_apriori.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Using sample of {sample_size:,} records ({sample_size/len(df_apriori)*100:.1f}% of total data)\")\n",
    "print(\"This reduces memory usage while maintaining statistical significance.\")\n",
    "\n",
    "# Create transactions with simpler item names (reduces unique items)\n",
    "transactions = []\n",
    "for idx, row in df_apriori_sample.iterrows():\n",
    "    transaction = []\n",
    "    for feature in features_for_apriori:\n",
    "        if pd.notna(row[feature]):\n",
    "            # Use simpler format: feature_value (no spaces, shorter)\n",
    "            value = str(row[feature]).replace(' ', '_').replace('-', '_')\n",
    "            transaction.append(f\"{feature}_{value}\")\n",
    "    transactions.append(transaction)\n",
    "\n",
    "# print(f\"\\nTotal transactions created: {len(transactions)}\")\n",
    "# print(f\"Sample transaction (first 5 items): {transactions[0][:5]}\")\n",
    "# print(f\"Average items per transaction: {np.mean([len(t) for t in transactions]):.2f}\")\n",
    "\n",
    "# Encode transactions\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# print(f\"\\nEncoded dataset shape: {df_encoded.shape}\")\n",
    "# print(f\"Total unique items: {len(te.columns_)}\")\n",
    "# print(f\"Memory usage reduced significantly!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa4d28",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "##### 4.4. Finding Frequent Itemsets using Apriori\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd48b81",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Apply Apriori algorithm to find frequent itemsets\n",
    "# Increased min_support to reduce memory usage and focus on more frequent patterns\n",
    "min_support = 0.05  # 5% of transactions (increased from 1% to reduce combinations)\n",
    "\n",
    "# print(\"Running Apriori Algorithm...\")\n",
    "# print(f\"Minimum Support: {min_support} ({min_support*100}% of transactions)\")\n",
    "# print(f\"Using low_memory mode to handle large dataset efficiently\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# Use low_memory=True and max_len to limit itemset size\n",
    "try:\n",
    "    frequent_itemsets = apriori(df_encoded, \n",
    "                                min_support=min_support, \n",
    "                                use_colnames=True, \n",
    "                                verbose=1,\n",
    "                                low_memory=True,\n",
    "                                max_len=4)  # Limit to 4-item itemsets max\n",
    "    \n",
    "    print(f\"\\n✓ Apriori algorithm completed!\")\n",
    "    print(f\"Total frequent itemsets found: {len(frequent_itemsets)}\")\n",
    "    \n",
    "    if len(frequent_itemsets) > 0:\n",
    "        print(f\"\\nTop 10 frequent itemsets by support:\")\n",
    "        print(frequent_itemsets.nlargest(10, 'support').to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\nNo frequent itemsets found. Try reducing min_support threshold.\")\n",
    "        \n",
    "except MemoryError:\n",
    "    print(\"\\n⚠ Memory error still occurring. Further optimizations needed:\")\n",
    "    print(\"   - Increasing min_support to 0.1 (10%)\")\n",
    "    print(\"   - Using even smaller sample size\")\n",
    "    \n",
    "    # Try with higher support\n",
    "    min_support = 0.1\n",
    "    print(f\"\\nRetrying with min_support = {min_support}...\")\n",
    "    frequent_itemsets = apriori(df_encoded, \n",
    "                                min_support=min_support, \n",
    "                                use_colnames=True, \n",
    "                                verbose=1,\n",
    "                                low_memory=True,\n",
    "                                max_len=3)  # Limit to 3-item itemsets\n",
    "    \n",
    "    print(f\"\\n✓ Apriori algorithm completed with higher support!\")\n",
    "    print(f\"Total frequent itemsets found: {len(frequent_itemsets)}\")\n",
    "    if len(frequent_itemsets) > 0:\n",
    "        print(f\"\\nTop 10 frequent itemsets by support:\")\n",
    "        print(frequent_itemsets.nlargest(10, 'support').to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8321ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize frequent itemsets\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Top 15 frequent itemsets\n",
    "top_itemsets = frequent_itemsets.nlargest(15, 'support')\n",
    "\n",
    "axes[0].barh(range(len(top_itemsets)), top_itemsets['support'], color='steelblue', alpha=0.7)\n",
    "axes[0].set_yticks(range(len(top_itemsets)))\n",
    "axes[0].set_yticklabels([', '.join(list(itemset)[:2]) + '...' if len(itemset) > 2 else ', '.join(list(itemset)) \n",
    "                          for itemset in top_itemsets['itemsets']], fontsize=8)\n",
    "axes[0].set_xlabel('Support', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Top 15 Frequent Itemsets by Support', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Itemset length distribution\n",
    "itemset_lengths = frequent_itemsets['itemsets'].apply(len)\n",
    "length_counts = itemset_lengths.value_counts().sort_index()\n",
    "\n",
    "axes[1].bar(length_counts.index, length_counts.values, color='coral', alpha=0.7)\n",
    "axes[1].set_xlabel('Itemset Length', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Number of Itemsets', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Distribution of Itemset Lengths', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, v in enumerate(length_counts.values):\n",
    "    axes[1].text(length_counts.index[i], v + 10, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nItemset Length Statistics:\")\n",
    "print(f\"  1-item itemsets: {len(frequent_itemsets[itemset_lengths == 1])}\")\n",
    "print(f\"  2-item itemsets: {len(frequent_itemsets[itemset_lengths == 2])}\")\n",
    "print(f\"  3-item itemsets: {len(frequent_itemsets[itemset_lengths == 3])}\")\n",
    "print(f\"  4+ item itemsets: {len(frequent_itemsets[itemset_lengths >= 4])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653ec9b2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate association rules from frequent itemsets\n",
    "# Check if we have frequent itemsets first\n",
    "if len(frequent_itemsets) == 0:\n",
    "    print(\" No frequent itemsets found. Cannot generate association rules.\")\n",
    "    print(\"Try reducing min_support threshold in the previous cell.\")\n",
    "    rules = pd.DataFrame()  # Empty dataframe\n",
    "else:\n",
    "    # min_threshold: minimum confidence threshold\n",
    "    min_confidence = 0.3\n",
    "\n",
    "    print(\"Generating Association Rules...\")\n",
    "    print(f\"Minimum Confidence: {min_confidence} ({min_confidence*100}%)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "\n",
    "    # Sort by confidence and lift\n",
    "    rules = rules.sort_values(['confidence', 'lift'], ascending=[False, False])\n",
    "\n",
    "    print(f\"\\n Association rules generated!\")\n",
    "    print(f\"Total rules found: {len(rules)}\")\n",
    "    \n",
    "    if len(rules) > 0:\n",
    "        print(f\"\\nTop 10 Association Rules by Confidence:\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # Display top rules in a readable format\n",
    "        top_rules = rules.head(10)\n",
    "        for idx, (rule_idx, rule) in enumerate(top_rules.iterrows(), 1):\n",
    "            antecedents = ', '.join([str(item).replace('_', ' ').title() for item in list(rule['antecedents'])])\n",
    "            consequents = ', '.join([str(item).replace('_', ' ').title() for item in list(rule['consequents'])])\n",
    "            print(f\"\\nRule {idx}:\")\n",
    "            print(f\"  IF {antecedents}\")\n",
    "            print(f\"  THEN {consequents}\")\n",
    "            print(f\"  Support: {rule['support']:.4f} | Confidence: {rule['confidence']:.4f} | Lift: {rule['lift']:.4f}\")\n",
    "    else:\n",
    "        print(\"No association rules found with current confidence threshold.\")\n",
    "        print(\"Try reducing min_confidence threshold.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0181ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Filter rules related to cancellation (only if rules exist)\n",
    "if len(rules) > 0:\n",
    "    # Helper function to check if consequents contain 'canceled' but not 'not_canceled'\n",
    "    def is_canceled_rule(consequents):\n",
    "        consequents_str = ' '.join([str(item).lower() for item in consequents])\n",
    "        return 'canceled' in consequents_str and 'not_canceled' not in consequents_str\n",
    "    \n",
    "    # Helper function to check if consequents contain 'not_canceled'\n",
    "    def is_not_canceled_rule(consequents):\n",
    "        consequents_str = ' '.join([str(item).lower() for item in consequents])\n",
    "        return 'not_canceled' in consequents_str\n",
    "    \n",
    "    canceled_rules = rules[rules['consequents'].apply(is_canceled_rule)]\n",
    "    not_canceled_rules = rules[rules['consequents'].apply(is_not_canceled_rule)]\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ASSOCIATION RULES RELATED TO CANCELLATION\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(f\"\\nRules leading to CANCELLATION (Top 10):\")\n",
    "    print(\"-\" * 80)\n",
    "    if len(canceled_rules) > 0:\n",
    "        top_canceled = canceled_rules.head(10)\n",
    "        for idx, (rule_idx, rule) in enumerate(top_canceled.iterrows(), 1):\n",
    "            antecedents = ', '.join([str(item).replace('_', ' ').title() for item in list(rule['antecedents'])])\n",
    "            print(f\"\\n{idx}. IF {antecedents}\")\n",
    "            print(f\"   THEN Canceled\")\n",
    "            print(f\"   Support: {rule['support']:.4f} | Confidence: {rule['confidence']:.4f} | Lift: {rule['lift']:.4f}\")\n",
    "    else:\n",
    "        print(\"No rules found leading to cancellation with current thresholds.\")\n",
    "\n",
    "    print(f\"\\n\\nRules leading to NO CANCELLATION (Top 10):\")\n",
    "    print(\"-\" * 80)\n",
    "    if len(not_canceled_rules) > 0:\n",
    "        top_not_canceled = not_canceled_rules.head(10)\n",
    "        for idx, (rule_idx, rule) in enumerate(top_not_canceled.iterrows(), 1):\n",
    "            antecedents = ', '.join([str(item).replace('_', ' ').title() for item in list(rule['antecedents'])])\n",
    "            print(f\"\\n{idx}. IF {antecedents}\")\n",
    "            print(f\"   THEN Not Canceled\")\n",
    "            print(f\"   Support: {rule['support']:.4f} | Confidence: {rule['confidence']:.4f} | Lift: {rule['lift']:.4f}\")\n",
    "    else:\n",
    "        print(\"No rules found leading to no cancellation with current thresholds.\")\n",
    "else:\n",
    "    print(\"No association rules available to filter.\")\n",
    "    canceled_rules = pd.DataFrame()\n",
    "    not_canceled_rules = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c9aad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "##### 4.6. Visualization of Association Rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d4109",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualization (only if we have rules)\n",
    "if len(rules) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    # Plot 1: Support vs Confidence\n",
    "    scatter1 = axes[0, 0].scatter(rules['support'], rules['confidence'], \n",
    "                       c=rules['lift'], cmap='viridis', alpha=0.6, s=50)\n",
    "    axes[0, 0].set_xlabel('Support', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Confidence', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_title('Support vs Confidence (colored by Lift)', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    cbar1 = plt.colorbar(scatter1, ax=axes[0, 0])\n",
    "    cbar1.set_label('Lift', fontsize=10)\n",
    "\n",
    "    # Plot 2: Confidence vs Lift\n",
    "    scatter2 = axes[0, 1].scatter(rules['confidence'], rules['lift'], \n",
    "                       c=rules['support'], cmap='plasma', alpha=0.6, s=50)\n",
    "    axes[0, 1].set_xlabel('Confidence', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Lift', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_title('Confidence vs Lift (colored by Support)', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    cbar2 = plt.colorbar(scatter2, ax=axes[0, 1])\n",
    "    cbar2.set_label('Support', fontsize=10)\n",
    "\n",
    "    # Plot 3: Top rules by confidence (for cancellation)\n",
    "    if len(canceled_rules) > 0:\n",
    "        top_canceled_viz = canceled_rules.head(10)\n",
    "        y_pos = np.arange(len(top_canceled_viz))\n",
    "        axes[1, 0].barh(y_pos, top_canceled_viz['confidence'], color='crimson', alpha=0.7)\n",
    "        axes[1, 0].set_yticks(y_pos)\n",
    "        # Use iterrows() instead of itertuples() to access columns by name\n",
    "        labels = []\n",
    "        for idx, rule in top_canceled_viz.iterrows():\n",
    "            antecedents = list(rule['antecedents'])\n",
    "            label = ', '.join([str(item).replace('_', ' ')[:15] for item in antecedents[:2]])\n",
    "            if len(antecedents) > 2:\n",
    "                label += '...'\n",
    "            labels.append(label[:40])\n",
    "        axes[1, 0].set_yticklabels(labels, fontsize=8)\n",
    "        axes[1, 0].set_xlabel('Confidence', fontsize=12, fontweight='bold')\n",
    "        axes[1, 0].set_title('Top 10 Rules Leading to Cancellation', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
    "        axes[1, 0].invert_yaxis()\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'No cancellation rules found', \n",
    "                        ha='center', va='center', fontsize=12)\n",
    "        axes[1, 0].set_title('Top 10 Rules Leading to Cancellation', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Plot 4: Top rules by confidence (for no cancellation)\n",
    "    if len(not_canceled_rules) > 0:\n",
    "        top_not_canceled_viz = not_canceled_rules.head(10)\n",
    "        y_pos = np.arange(len(top_not_canceled_viz))\n",
    "        axes[1, 1].barh(y_pos, top_not_canceled_viz['confidence'], color='forestgreen', alpha=0.7)\n",
    "        axes[1, 1].set_yticks(y_pos)\n",
    "        # Use iterrows() instead of itertuples() to access columns by name\n",
    "        labels = []\n",
    "        for idx, rule in top_not_canceled_viz.iterrows():\n",
    "            antecedents = list(rule['antecedents'])\n",
    "            label = ', '.join([str(item).replace('_', ' ')[:15] for item in antecedents[:2]])\n",
    "            if len(antecedents) > 2:\n",
    "                label += '...'\n",
    "            labels.append(label[:40])\n",
    "        axes[1, 1].set_yticklabels(labels, fontsize=8)\n",
    "        axes[1, 1].set_xlabel('Confidence', fontsize=12, fontweight='bold')\n",
    "        axes[1, 1].set_title('Top 10 Rules Leading to No Cancellation', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "        axes[1, 1].invert_yaxis()\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'No no-cancellation rules found', \n",
    "                        ha='center', va='center', fontsize=12)\n",
    "        axes[1, 1].set_title('Top 10 Rules Leading to No Cancellation', fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.suptitle('Association Rules Analysis - Apriori Algorithm', fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No rules available for visualization. Please ensure frequent itemsets were found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ea026a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "##### 4.7. Key Insights from Apriori Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8350b521",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\" * 80)\n",
    "print(\"APRIORI ALGORITHM SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n1. Data Processing:\")\n",
    "print(f\"   - Sample size used: {sample_size:,} records ({sample_size/len(df_apriori)*100:.1f}% of total)\")\n",
    "print(f\"   - Features analyzed: {len(features_for_apriori)}\")\n",
    "print(f\"   - Unique items in transactions: {len(te.columns_)}\")\n",
    "\n",
    "print(f\"\\n2. Frequent Itemsets:\")\n",
    "if len(frequent_itemsets) > 0:\n",
    "    print(f\"   - Total frequent itemsets found: {len(frequent_itemsets)}\")\n",
    "    print(f\"   - Minimum support threshold: {min_support} ({min_support*100}%)\")\n",
    "else:\n",
    "    print(f\"   - No frequent itemsets found with min_support = {min_support}\")\n",
    "    print(f\"   - Try reducing min_support threshold\")\n",
    "\n",
    "if len(rules) > 0:\n",
    "    print(f\"\\n3. Association Rules:\")\n",
    "    print(f\"   - Total rules generated: {len(rules)}\")\n",
    "    print(f\"   - Minimum confidence threshold: {min_confidence} ({min_confidence*100}%)\")\n",
    "    print(f\"   - Average confidence: {rules['confidence'].mean():.4f}\")\n",
    "    print(f\"   - Average lift: {rules['lift'].mean():.4f}\")\n",
    "    print(f\"   - Maximum lift: {rules['lift'].max():.4f}\")\n",
    "\n",
    "    print(f\"\\n4. Cancellation-Related Rules:\")\n",
    "    print(f\"   - Rules leading to cancellation: {len(canceled_rules)}\")\n",
    "    if len(canceled_rules) > 0:\n",
    "        print(f\"   - Average confidence: {canceled_rules['confidence'].mean():.4f}\")\n",
    "        print(f\"   - Average lift: {canceled_rules['lift'].mean():.4f}\")\n",
    "\n",
    "    print(f\"   - Rules leading to no cancellation: {len(not_canceled_rules)}\")\n",
    "    if len(not_canceled_rules) > 0:\n",
    "        print(f\"   - Average confidence: {not_canceled_rules['confidence'].mean():.4f}\")\n",
    "        print(f\"   - Average lift: {not_canceled_rules['lift'].mean():.4f}\")\n",
    "else:\n",
    "    print(f\"\\n3. Association Rules:\")\n",
    "    print(f\"   - No rules generated (no frequent itemsets found)\")\n",
    "\n",
    "print(f\"\\n5. Key Metrics Explained:\")\n",
    "print(f\"   - Support: Frequency of itemset in transactions\")\n",
    "print(f\"   - Confidence: Probability of consequent given antecedent\")\n",
    "print(f\"   - Lift: How much more likely consequent is with antecedent\")\n",
    "print(f\"     (Lift > 1: Positive association, Lift < 1: Negative association)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Apriori algorithm analysis completed!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d973cd3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\" * 80)\n",
    "print(\"APRIORI ALGORITHM SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n1. Data Processing:\")\n",
    "print(f\"   - Sample size used: {sample_size:,} records ({sample_size/len(df_apriori)*100:.1f}% of total)\")\n",
    "print(f\"   - Features analyzed: {len(features_for_apriori)}\")\n",
    "print(f\"   - Unique items in transactions: {len(te.columns_)}\")\n",
    "\n",
    "print(f\"\\n2. Frequent Itemsets:\")\n",
    "if len(frequent_itemsets) > 0:\n",
    "    print(f\"   - Total frequent itemsets found: {len(frequent_itemsets)}\")\n",
    "    print(f\"   - Minimum support threshold: {min_support} ({min_support*100}%)\")\n",
    "else:\n",
    "    print(f\"   - No frequent itemsets found with min_support = {min_support}\")\n",
    "    print(f\"   - Try reducing min_support threshold\")\n",
    "\n",
    "if len(rules) > 0:\n",
    "    print(f\"\\n3. Association Rules:\")\n",
    "    print(f\"   - Total rules generated: {len(rules)}\")\n",
    "    print(f\"   - Minimum confidence threshold: {min_confidence} ({min_confidence*100}%)\")\n",
    "    print(f\"   - Average confidence: {rules['confidence'].mean():.4f}\")\n",
    "    print(f\"   - Average lift: {rules['lift'].mean():.4f}\")\n",
    "    print(f\"   - Maximum lift: {rules['lift'].max():.4f}\")\n",
    "\n",
    "    print(f\"\\n4. Cancellation-Related Rules:\")\n",
    "    print(f\"   - Rules leading to cancellation: {len(canceled_rules)}\")\n",
    "    if len(canceled_rules) > 0:\n",
    "        print(f\"   - Average confidence: {canceled_rules['confidence'].mean():.4f}\")\n",
    "        print(f\"   - Average lift: {canceled_rules['lift'].mean():.4f}\")\n",
    "\n",
    "    print(f\"   - Rules leading to no cancellation: {len(not_canceled_rules)}\")\n",
    "    if len(not_canceled_rules) > 0:\n",
    "        print(f\"   - Average confidence: {not_canceled_rules['confidence'].mean():.4f}\")\n",
    "        print(f\"   - Average lift: {not_canceled_rules['lift'].mean():.4f}\")\n",
    "else:\n",
    "    print(f\"\\n3. Association Rules:\")\n",
    "    print(f\"   - No rules generated (no frequent itemsets found)\")\n",
    "\n",
    "print(f\"\\n5. Key Metrics Explained:\")\n",
    "print(f\"   - Support: Frequency of itemset in transactions\")\n",
    "print(f\"   - Confidence: Probability of consequent given antecedent\")\n",
    "print(f\"   - Lift: How much more likely consequent is with antecedent\")\n",
    "print(f\"     (Lift > 1: Positive association, Lift < 1: Negative association)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Apriori algorithm analysis completed!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0a99e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Select numeric columns for outlier detection (excluding target variable and year)\n",
    "numeric_cols = ['lead_time', 'stays_in_weekend_nights', 'stays_in_week_nights', \n",
    "                'adults', 'children', 'is_repeated_guest', 'previous_cancellations',\n",
    "                'previous_bookings_not_canceled', 'booking_changes', \n",
    "                'days_in_waiting_list', 'avg_daily_rate', \n",
    "                'required_car_parking_spaces', 'total_of_special_requests']\n",
    "\n",
    "print(\"Original dataset shape:\", df.shape)\n",
    "print(f\"Number of numeric columns for outlier detection: {len(numeric_cols)}\")\n",
    "print(f\"Columns: {', '.join(numeric_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddc5d50",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "##### 3.1. IQR-Based Outlier Detection and Treatment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f376ddcf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def detect_and_treat_outliers_iqr(df, columns):\n",
    "    \n",
    "    # Detect and treat outliers using IQR method with Winsorization (capping)\n",
    "    # This is the best method for hotel booking data as it:\n",
    "    # - Is robust to outliers (uses median-based quartiles)\n",
    "    # - Preserves all data points (caps instead of removing)\n",
    "    # - Works well with skewed distributions\n",
    "    # - Prevents data loss\n",
    "    \n",
    "    # Returns: cleaned dataframe and summary statistics\n",
    "    \n",
    "    df_cleaned = df.copy()\n",
    "    summary = {}\n",
    "    total_outliers = 0\n",
    "    \n",
    "    print(\"IQR-Based Outlier Detection and Treatment (Winsorization)\")\n",
    "    print(\"=\" * 90)\n",
    "    print(f\"{'Column':<30} {'Outliers':<15} {'Percentage':<15} {'Action':<20}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for col in columns:\n",
    "        # Calculate IQR\n",
    "        Q1 = df_cleaned[col].quantile(0.25)\n",
    "        Q3 = df_cleaned[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Calculate bounds (1.5 * IQR is standard)\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Ensure lower bound is not negative for count-based features\n",
    "        if col in ['adults', 'children', 'stays_in_weekend_nights', 'stays_in_week_nights',\n",
    "                   'previous_cancellations', 'previous_bookings_not_canceled', \n",
    "                   'booking_changes', 'required_car_parking_spaces', \n",
    "                   'total_of_special_requests', 'days_in_waiting_list']:\n",
    "            lower_bound = max(0, lower_bound)\n",
    "        \n",
    "        # Count outliers before treatment\n",
    "        outliers_before = ((df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound)).sum()\n",
    "        outlier_pct = (outliers_before / len(df_cleaned)) * 100\n",
    "        total_outliers += outliers_before\n",
    "        \n",
    "        # Cap outliers (Winsorization)\n",
    "        df_cleaned[col] = np.clip(df_cleaned[col], lower_bound, upper_bound)\n",
    "        \n",
    "        # Store summary\n",
    "        summary[col] = {\n",
    "            'outliers_before': outliers_before,\n",
    "            'outlier_percentage': outlier_pct,\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound,\n",
    "            'Q1': Q1,\n",
    "            'Q3': Q3,\n",
    "            'IQR': IQR\n",
    "        }\n",
    "        \n",
    "        # Print summary for this column\n",
    "        action = f\"Capped to [{lower_bound:.1f}, {upper_bound:.1f}]\"\n",
    "        print(f\"{col:<30} {outliers_before:<15} {outlier_pct:<14.2f}% {action:<20}\")\n",
    "    \n",
    "    print(\"-\" * 90)\n",
    "    print(f\"{'TOTAL OUTLIERS TREATED':<30} {total_outliers:<15} {(total_outliers/len(df)*100):<14.2f}%\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    return df_cleaned, summary\n",
    "\n",
    "# Apply outlier detection and treatment\n",
    "df_cleaned, outlier_summary = detect_and_treat_outliers_iqr(df, numeric_cols)\n",
    "\n",
    "print(f\"\\n✓ Outlier treatment completed!\")\n",
    "print(f\"Dataset shape: {df.shape} → {df_cleaned.shape} (no rows removed)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a92e66",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "##### 3.2. Visualization of Outliers (Before Treatment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c165c1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize outliers in key columns using boxplots\n",
    "key_cols = ['lead_time', 'avg_daily_rate', 'stays_in_weekend_nights', \n",
    "            'stays_in_week_nights', 'days_in_waiting_list', 'previous_cancellations']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(key_cols):\n",
    "    # Create boxplot\n",
    "    bp = axes[idx].boxplot(df[col], vert=True, patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    axes[idx].set_title(f'Boxplot of {col}\\n(Outliers shown as points)', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Value', fontsize=10)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add outlier count annotation\n",
    "    info = outlier_summary[col]\n",
    "    axes[idx].text(0.5, 0.95, f\"Outliers: {info['outliers_before']} ({info['outlier_percentage']:.1f}%)\",\n",
    "                   transform=axes[idx].transAxes, ha='center', va='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "                   fontsize=9)\n",
    "\n",
    "plt.suptitle('Outlier Visualization - Before Treatment', fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad11a80c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "##### 3.3. Statistical Comparison: Before vs After Treatment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5558a87f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Compare statistics before and after outlier treatment\n",
    "comparison_cols = ['lead_time', 'avg_daily_rate', 'stays_in_weekend_nights', \n",
    "                   'stays_in_week_nights', 'days_in_waiting_list', 'previous_cancellations']\n",
    "\n",
    "print(\"\\nStatistical Comparison: Before vs After Outlier Treatment\")\n",
    "print(\"=\" * 110)\n",
    "print(f\"{'Column':<25} {'Metric':<15} {'Before':<15} {'After':<15} {'Change':<15} {'% Change':<15}\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "for col in comparison_cols:\n",
    "    before_mean = df[col].mean()\n",
    "    after_mean = df_cleaned[col].mean()\n",
    "    before_std = df[col].std()\n",
    "    after_std = df_cleaned[col].std()\n",
    "    before_max = df[col].max()\n",
    "    after_max = df_cleaned[col].max()\n",
    "    before_median = df[col].median()\n",
    "    after_median = df_cleaned[col].median()\n",
    "    \n",
    "    print(f\"\\n{col.upper()}\")\n",
    "    print(f\"{'':<25} {'Mean':<15} {before_mean:<15.2f} {after_mean:<15.2f} {after_mean-before_mean:<15.2f} {((after_mean-before_mean)/before_mean*100):<14.2f}%\")\n",
    "    print(f\"{'':<25} {'Std Dev':<15} {before_std:<15.2f} {after_std:<15.2f} {after_std-before_std:<15.2f} {((after_std-before_std)/before_std*100):<14.2f}%\")\n",
    "    print(f\"{'':<25} {'Median':<15} {before_median:<15.2f} {after_median:<15.2f} {after_median-before_median:<15.2f} {((after_median-before_median)/before_median*100):<14.2f}%\")\n",
    "    print(f\"{'':<25} {'Max':<15} {before_max:<15.2f} {after_max:<15.2f} {after_max-before_max:<15.2f} {((after_max-before_max)/before_max*100):<14.2f}%\")\n",
    "\n",
    "print(\"=\" * 110)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f607ce5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "##### 3.4. Visualization: After Treatment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dd6975",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the cleaned data to show the effect of outlier treatment\n",
    "key_cols = ['lead_time', 'avg_daily_rate', 'stays_in_weekend_nights', \n",
    "            'stays_in_week_nights', 'days_in_waiting_list', 'previous_cancellations']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(key_cols):\n",
    "    # Create boxplot for cleaned data\n",
    "    bp = axes[idx].boxplot(df_cleaned[col], vert=True, patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('lightgreen')\n",
    "    axes[idx].set_title(f'Boxplot of {col}\\n(After Outlier Treatment)', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Value', fontsize=10)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add summary annotation\n",
    "    info = outlier_summary[col]\n",
    "    axes[idx].text(0.5, 0.95, f\"Outliers treated: {info['outliers_before']}\",\n",
    "                   transform=axes[idx].transAxes, ha='center', va='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5),\n",
    "                   fontsize=9)\n",
    "\n",
    "plt.suptitle('Outlier Visualization - After Treatment (Winsorization)', fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af87d16",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "##### 3.5. Update Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdd8b03",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Update the main dataframe to use cleaned data\n",
    "df = df_cleaned.copy()\n",
    "\n",
    "print(\"✓ Dataset updated with outlier-treated data\")\n",
    "print(f\"Final dataset shape: {df.shape}\")\n",
    "print(f\"✓ All outliers have been treated using IQR Winsorization method\")\n",
    "print(f\"✓ Data is now ready for model building\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d4aa16",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "##### Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0411de6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Display final summary\n",
    "total_treated = sum([info['outliers_before'] for info in outlier_summary.values()])\n",
    "total_pct = (total_treated / len(df)) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"OUTLIER DETECTION AND TREATMENT SUMMARY\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"Method Used: IQR (Interquartile Range) with Winsorization\")\n",
    "print(f\"Total Outliers Treated: {total_treated:,} ({total_pct:.2f}% of dataset)\")\n",
    "print(f\"Treatment Method: Capping (preserves all data points)\")\n",
    "print(f\"Dataset Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(\"\\nWhy IQR Method?\")\n",
    "print(\"  ✓ Robust to outliers (uses quartiles, not mean/std)\")\n",
    "print(\"  ✓ Preserves all data (no rows removed)\")\n",
    "print(\"  ✓ Works well with skewed distributions\")\n",
    "print(\"  ✓ Industry standard for hotel/booking data\")\n",
    "print(\"  ✓ Prevents extreme values from skewing the model\")\n",
    "print(\"=\" * 90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c584abd1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "##### Model Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c056f95f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "##### 1. Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ed402",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eecf8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 2. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf028d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53431d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5cad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd023a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 3. Labeling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765783eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = df.select_dtypes(include=[np.number])\n",
    "df_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b563dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df.select_dtypes(include=['object'])\n",
    "df_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c5b914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "df_cat = df_cat.apply(LabelEncoder().fit_transform)\n",
    "df_cat.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50236705",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_num, df_cat], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d923204",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 3. Data Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c8eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(['is_canceled'], axis = 1)\n",
    "y = df['is_canceled']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d35ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d827c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 4. Model Building (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf94d97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Grid Search method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662e41fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = {'min_samples_split': [50, 75, 150, 250], \n",
    "         'min_samples_leaf': [10, 30, 50, 70],\n",
    "         'max_depth': np.arange(2, 7, 2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084a345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bddfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
    "                           paras,\n",
    "                           verbose=1,\n",
    "                           cv=10)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96395798",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6469333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "Model = RandomForestClassifier(n_estimators=25,\n",
    "                               criterion='gini',\n",
    "                               max_depth=6,\n",
    "                               min_samples_leaf=10,\n",
    "                               min_samples_split=50,\n",
    "                               max_features='sqrt')\n",
    "Model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b7cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ce590",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = pd.Series(data = Model.feature_importances_, index = Model.feature_names_in_).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"Feature Imprtance / Selection\")\n",
    "ax = sns.barplot(y = imp.index, x = imp.values, palette = 'BrBG', orient = 'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2180047",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Random Forest Visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b6cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adaaa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc726f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "feature_list = list(X.columns)\n",
    "canceled = ['No', 'Yes']\n",
    "\n",
    "# pick one tree from the RandomForest\n",
    "tree = Model.estimators_[10]\n",
    "\n",
    "plt.figure(figsize=(25, 12))\n",
    "plot_tree(\n",
    "    tree,\n",
    "    feature_names=feature_list,\n",
    "    class_names=canceled,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=8\n",
    ")\n",
    "\n",
    "plt.savefig(\"project_tree.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 5. Prediction on Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54ed758",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6992dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Predicted'] = Model.predict(X_train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55fa804",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 6. Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04413d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(train['is_canceled'], train['Predicted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea45f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 7. Predictions on Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e43c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cd4dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Predicted'] = Model.predict(X_test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c264a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 8. Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbec8e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test['is_canceled'], test['Predicted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55cbfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(Model, open(r'build.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
